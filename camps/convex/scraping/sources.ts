import { mutation } from '../_generated/server';
import { v } from 'convex/values';
import { getBuiltInScraperForUrl } from './scraperCodeValidation';

// Validators for scraper config matching schema.ts
const scraperConfigValidator = v.object({
  version: v.number(),
  generatedAt: v.number(),
  generatedBy: v.union(v.literal('claude'), v.literal('manual')),

  entryPoints: v.array(
    v.object({
      url: v.string(),
      type: v.union(v.literal('session_list'), v.literal('calendar'), v.literal('program_page')),
    }),
  ),

  pagination: v.optional(
    v.object({
      type: v.union(v.literal('next_button'), v.literal('load_more'), v.literal('page_numbers'), v.literal('none')),
      selector: v.optional(v.string()),
    }),
  ),

  sessionExtraction: v.object({
    containerSelector: v.string(),
    fields: v.object({
      name: v.object({ selector: v.string() }),
      dates: v.object({ selector: v.string(), format: v.string() }),
      price: v.optional(v.object({ selector: v.string() })),
      ageRange: v.optional(v.object({ selector: v.string(), pattern: v.string() })),
      status: v.optional(
        v.object({
          selector: v.string(),
          soldOutIndicators: v.array(v.string()),
        }),
      ),
      registrationUrl: v.optional(v.object({ selector: v.string() })),
    }),
  }),

  requiresJavaScript: v.boolean(),
  waitForSelector: v.optional(v.string()),
});

/**
 * Create a new scrape source
 */
export const createScrapeSource = mutation({
  args: {
    name: v.string(),
    url: v.string(),
    cityId: v.id('cities'), // Required: market this source belongs to
    organizationId: v.optional(v.id('organizations')),
    scraperConfig: scraperConfigValidator,
    scrapeFrequencyHours: v.number(),
  },
  handler: async (ctx, args) => {
    // Validate organization exists if provided
    if (args.organizationId) {
      const org = await ctx.db.get(args.organizationId);
      if (!org) {
        throw new Error('Organization not found');
      }
    }

    // Validate scrape frequency
    if (args.scrapeFrequencyHours < 1) {
      throw new Error('Scrape frequency must be at least 1 hour');
    }

    const now = Date.now();

    const sourceId = await ctx.db.insert('scrapeSources', {
      name: args.name,
      url: args.url,
      cityId: args.cityId,
      organizationId: args.organizationId,
      scraperConfig: args.scraperConfig,
      scraperHealth: {
        lastSuccessAt: undefined,
        lastFailureAt: undefined,
        consecutiveFailures: 0,
        totalRuns: 0,
        successRate: 0,
        lastError: undefined,
        needsRegeneration: false,
      },
      scrapeFrequencyHours: args.scrapeFrequencyHours,
      lastScrapedAt: undefined,
      nextScheduledScrape: now, // Schedule immediately
      isActive: true,
    });

    // Create initial version history
    await ctx.db.insert('scraperVersions', {
      scrapeSourceId: sourceId,
      version: args.scraperConfig.version,
      config: JSON.stringify(args.scraperConfig),
      createdAt: now,
      createdBy: args.scraperConfig.generatedBy,
      changeReason: 'Initial configuration',
      isActive: true,
    });

    return sourceId;
  },
});

/**
 * Create a simple scrape source without scraper config
 * Used by market seeding - the daemon will generate the scraper
 *
 * Automatically assigns built-in scrapers if the URL matches a known domain.
 */
export const createScrapeSourceSimple = mutation({
  args: {
    name: v.string(),
    url: v.string(),
    organizationId: v.id('organizations'),
  },
  handler: async (ctx, args) => {
    // Validate organization exists
    const org = await ctx.db.get(args.organizationId);
    if (!org) {
      throw new Error('Organization not found');
    }

    // Get cityId from organization
    if (!org.cityIds || org.cityIds.length === 0) {
      throw new Error('Organization has no city assigned');
    }
    const cityId = org.cityIds[0];

    // Check if source already exists for this URL
    const existing = await ctx.db
      .query('scrapeSources')
      .withIndex('by_url', (q) => q.eq('url', args.url))
      .first();

    if (existing) {
      return existing._id;
    }

    // Auto-detect built-in scraper based on domain
    const builtInScraper = getBuiltInScraperForUrl(args.url);

    const sourceId = await ctx.db.insert('scrapeSources', {
      name: args.name,
      url: args.url,
      cityId,
      organizationId: args.organizationId,
      // Auto-assign built-in scraper if URL matches known domain
      scraperModule: builtInScraper,
      // scraperConfig and scraperCode will be generated by daemon (if no built-in)
      scraperHealth: {
        lastSuccessAt: undefined,
        lastFailureAt: undefined,
        consecutiveFailures: 0,
        totalRuns: 0,
        successRate: 0,
        lastError: undefined,
        needsRegeneration: false,
      },
      scrapeFrequencyHours: 24, // Daily by default
      lastScrapedAt: undefined,
      // Schedule immediately if we have a built-in scraper
      nextScheduledScrape: builtInScraper ? Date.now() : undefined,
      isActive: true,
    });

    // If no built-in scraper, create a development request so daemon can write one
    if (!builtInScraper) {
      // Check if dev request already exists for this URL
      const existingDevRequest = await ctx.db
        .query('scraperDevelopmentRequests')
        .withIndex('by_source_url', (q) => q.eq('sourceUrl', args.url))
        .first();

      if (!existingDevRequest) {
        await ctx.db.insert('scraperDevelopmentRequests', {
          sourceName: args.name,
          sourceUrl: args.url,
          cityId,
          sourceId,
          requestedBy: 'auto-source-creation',
          requestedAt: Date.now(),
          status: 'pending',
          scraperVersion: 0,
        });
      }
    }

    return sourceId;
  },
});

/**
 * Update source details (name and/or URL)
 * Used for inline editing in Control Center
 */
export const updateSourceDetails = mutation({
  args: {
    sourceId: v.id('scrapeSources'),
    name: v.optional(v.string()),
    url: v.optional(v.string()),
  },
  handler: async (ctx, args) => {
    const source = await ctx.db.get(args.sourceId);
    if (!source) {
      throw new Error('Scrape source not found');
    }

    const updates: Record<string, string> = {};
    if (args.name !== undefined) {
      updates.name = args.name;
    }
    if (args.url !== undefined) {
      updates.url = args.url;
    }

    if (Object.keys(updates).length > 0) {
      await ctx.db.patch(args.sourceId, updates);
    }

    return args.sourceId;
  },
});

/**
 * Update the main URL of a source
 */
export const updateSourceUrl = mutation({
  args: {
    sourceId: v.id('scrapeSources'),
    url: v.string(),
  },
  handler: async (ctx, args) => {
    const source = await ctx.db.get(args.sourceId);
    if (!source) {
      throw new Error('Scrape source not found');
    }

    await ctx.db.patch(args.sourceId, {
      url: args.url,
    });

    return args.sourceId;
  },
});

/**
 * Activate a scrape source (enable scheduled scraping)
 */
export const activateScrapeSource = mutation({
  args: {
    sourceId: v.id('scrapeSources'),
  },
  handler: async (ctx, args) => {
    const source = await ctx.db.get(args.sourceId);
    if (!source) {
      throw new Error('Scrape source not found');
    }

    // Require either scraperModule or scraperCode
    if (!source.scraperModule && !source.scraperCode) {
      throw new Error('Cannot activate source without scraper code or module');
    }

    await ctx.db.patch(args.sourceId, {
      isActive: true,
      nextScheduledScrape: Date.now() + source.scrapeFrequencyHours * 60 * 60 * 1000,
    });

    return args.sourceId;
  },
});

/**
 * Enable or disable a scrape source
 */
export const toggleSourceActive = mutation({
  args: {
    sourceId: v.id('scrapeSources'),
    isActive: v.boolean(),
  },
  handler: async (ctx, args) => {
    const source = await ctx.db.get(args.sourceId);
    if (!source) {
      throw new Error('Scrape source not found');
    }

    await ctx.db.patch(args.sourceId, {
      isActive: args.isActive,
    });

    // Create alert when disabling a source
    if (!args.isActive && source.isActive) {
      await ctx.db.insert('scraperAlerts', {
        sourceId: args.sourceId,
        alertType: 'scraper_disabled',
        message: `Scraper "${source.name}" has been disabled.`,
        severity: 'info',
        createdAt: Date.now(),
        acknowledgedAt: undefined,
        acknowledgedBy: undefined,
      });
    }

    return args.sourceId;
  },
});

/**
 * Toggle a scrape source's active status
 */
export const setSourceActive = mutation({
  args: {
    sourceId: v.id('scrapeSources'),
    isActive: v.boolean(),
  },
  handler: async (ctx, args) => {
    await ctx.db.patch(args.sourceId, { isActive: args.isActive });
    return args.sourceId;
  },
});

/**
 * Mark a source as closed (no camps available)
 * Used by the daemon when it discovers a source doesn't actually offer camps
 * e.g., "Central org site - individual clubs may have camps on their own sites"
 */
export const markSourceClosed = mutation({
  args: {
    sourceId: v.id('scrapeSources'),
    reason: v.string(),
    closedBy: v.optional(v.string()), // "daemon" or admin identifier
  },
  handler: async (ctx, args) => {
    const source = await ctx.db.get(args.sourceId);
    if (!source) {
      throw new Error('Source not found');
    }

    await ctx.db.patch(args.sourceId, {
      isActive: false,
      closureReason: args.reason,
      closedAt: Date.now(),
      closedBy: args.closedBy ?? 'daemon',
    });

    return { closed: true, sourceId: args.sourceId };
  },
});

/**
 * Reopen a previously closed source
 */
export const reopenSource = mutation({
  args: {
    sourceId: v.id('scrapeSources'),
  },
  handler: async (ctx, args) => {
    const source = await ctx.db.get(args.sourceId);
    if (!source) {
      throw new Error('Source not found');
    }

    await ctx.db.patch(args.sourceId, {
      isActive: true,
      closureReason: undefined,
      closedAt: undefined,
      closedBy: undefined,
    });

    return { reopened: true, sourceId: args.sourceId };
  },
});

/**
 * Delete a source and optionally its associated data
 */
export const deleteSourceWithData = mutation({
  args: {
    sourceId: v.id('scrapeSources'),
    deleteJobs: v.optional(v.boolean()), // Default: true
    deleteSessions: v.optional(v.boolean()), // Default: false (dangerous)
  },
  handler: async (ctx, args) => {
    const source = await ctx.db.get(args.sourceId);
    if (!source) {
      throw new Error('Scrape source not found');
    }

    const deleteJobs = args.deleteJobs !== false; // Default true
    const deleteSessions = args.deleteSessions === true; // Default false

    // Delete associated scrape jobs
    if (deleteJobs) {
      const jobs = await ctx.db
        .query('scrapeJobs')
        .withIndex('by_source', (q) => q.eq('sourceId', args.sourceId))
        .collect();

      for (const job of jobs) {
        // Delete raw data for each job
        const rawData = await ctx.db
          .query('scrapeRawData')
          .withIndex('by_job', (q) => q.eq('jobId', job._id))
          .collect();
        for (const raw of rawData) {
          await ctx.db.delete(raw._id);
        }
        await ctx.db.delete(job._id);
      }
    }

    // Delete pending sessions
    const pendingSessions = await ctx.db
      .query('pendingSessions')
      .withIndex('by_source', (q) => q.eq('sourceId', args.sourceId))
      .collect();
    for (const pending of pendingSessions) {
      await ctx.db.delete(pending._id);
    }

    // Delete alerts for this source
    const alerts = await ctx.db
      .query('scraperAlerts')
      .withIndex('by_source', (q) => q.eq('sourceId', args.sourceId))
      .collect();
    for (const alert of alerts) {
      await ctx.db.delete(alert._id);
    }

    // Delete scraper versions
    const versions = await ctx.db
      .query('scraperVersions')
      .withIndex('by_source', (q) => q.eq('scrapeSourceId', args.sourceId))
      .collect();
    for (const version of versions) {
      await ctx.db.delete(version._id);
    }

    // Delete scrape changes
    const changes = await ctx.db
      .query('scrapeChanges')
      .withIndex('by_source', (q) => q.eq('sourceId', args.sourceId))
      .collect();
    for (const change of changes) {
      await ctx.db.delete(change._id);
    }

    // Optionally delete sessions (dangerous!)
    if (deleteSessions) {
      const sessions = await ctx.db
        .query('sessions')
        .withIndex('by_source', (q) => q.eq('sourceId', args.sourceId))
        .collect();
      for (const session of sessions) {
        await ctx.db.delete(session._id);
      }
    } else {
      // Just unlink sessions from this source
      const sessions = await ctx.db
        .query('sessions')
        .withIndex('by_source', (q) => q.eq('sourceId', args.sourceId))
        .collect();
      for (const session of sessions) {
        await ctx.db.patch(session._id, { sourceId: undefined });
      }
    }

    // Delete the source itself
    await ctx.db.delete(args.sourceId);

    return { deleted: true };
  },
});
